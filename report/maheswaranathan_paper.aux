\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{barlow}
\citation{petrov}
\citation{chandlerfield}
\citation{entropyest}
\citation{maxen}
\citation{nis}
\citation{bethge}
\citation{nicv1}
\citation{barlow}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{brf}{\backcite{barlow}{{1}{1}{section.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}\hskip -1em.\nobreakspace  {}Motivation and significance for computer vision}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}\hskip -1em.\nobreakspace  {}Background}{1}{subsection.1.2}}
\@writefile{brf}{\backcite{petrov,chandlerfield}{{1}{1.2}{subsection.1.2}}}
\@writefile{brf}{\backcite{entropyest}{{1}{1.2}{subsection.1.2}}}
\@writefile{brf}{\backcite{maxen}{{1}{1.2}{subsection.1.2}}}
\@writefile{brf}{\backcite{nis}{{1}{1.2}{subsection.1.2}}}
\@writefile{brf}{\backcite{bethge,nicv1}{{1}{1.2}{subsection.1.2}}}
\citation{vanhateren}
\citation{ica}
\citation{nis}
\citation{entropyest}
\@writefile{brf}{\backcite{barlow}{{2}{1.2}{subsection.1.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Methods}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}Dataset}{2}{subsection.2.1}}
\@writefile{brf}{\backcite{vanhateren}{{2}{2.1}{subsection.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}Reduced descriptions of images}{2}{subsection.2.2}}
\@writefile{brf}{\backcite{ica}{{2}{2.2}{figure.2}}}
\@writefile{brf}{\backcite{nis}{{2}{2.2}{figure.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Learned basis vectors using algorithms for redundancy reduction}}{2}{figure.2}}
\newlabel{fig:bases}{{2}{2}{Learned basis vectors using algorithms for redundancy reduction}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\hskip -1em.\nobreakspace  {}Entropy Estimation}{2}{subsection.2.3}}
\@writefile{brf}{\backcite{entropyest}{{2}{2.3}{equation.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}\hskip -1em.\nobreakspace  {}Computational complexity}{3}{subsection.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Results}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Single pixel and Pairwise statistics}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Evaluating the metric}{3}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Maximum entropy}{3}{subsection.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces N-dimensional entropy estimation. The nearest neighbor method converges to the true entropy (for Gaussian distributions) quickly with increasing sample sizes.}}{3}{figure.5}}
\newlabel{fig:ND}{{5}{3}{N-dimensional entropy estimation. The nearest neighbor method converges to the true entropy (for Gaussian distributions) quickly with increasing sample sizes}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.\nobreakspace  {}Entropy of raw image patches}{3}{subsection.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Entropy of Gaussian models of image patches. A Gaussian model of image patches was fit using 5000 samples for different patch sizes, the entropy of the Gaussian is then computed analytically given the covariance matrix.}}{4}{figure.6}}
\newlabel{fig:gaussian}{{6}{4}{Entropy of Gaussian models of image patches. A Gaussian model of image patches was fit using 5000 samples for different patch sizes, the entropy of the Gaussian is then computed analytically given the covariance matrix}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}\hskip -1em.\nobreakspace  {}Projection onto basis vectors}{4}{subsection.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}\hskip -1em.\nobreakspace  {}Entropy of the joint distribution of projections onto oriented filters}{4}{subsection.3.6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Conclusions and Future Work}{4}{section.4}}
\bibstyle{ieee}
\bibdata{egbib}
\bibcite{barlow}{1}
\bibcite{ica}{2}
\bibcite{bethge}{3}
\bibcite{maxen}{4}
\bibcite{chandlerfield}{5}
\bibcite{nicv1}{6}
\bibcite{entropyest}{7}
\bibcite{nis}{8}
\bibcite{petrov}{9}
\bibcite{vanhateren}{10}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Appendix}{5}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Sample natural image patches (top) and probability distribution over single pixel values (bottom)}}{6}{figure.1}}
\newlabel{fig:ex}{{1}{6}{Sample natural image patches (top) and probability distribution over single pixel values (bottom)}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Pairwise correlations between pixels}}{6}{figure.3}}
\newlabel{fig:pairwise}{{3}{6}{Pairwise correlations between pixels}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces 1D entropy estimation. Estimation using kernel density (red) and nearest neighbor (blue) methods, for samples drawn from a Gaussian (top) and Laplacian (bottom) as a function of the number of samples. Error bars are the variance in the estimate over 25 samples. The red/blue points are shown slightly offset for clarity.}}{7}{figure.4}}
\newlabel{fig:1D}{{4}{7}{1D entropy estimation. Estimation using kernel density (red) and nearest neighbor (blue) methods, for samples drawn from a Gaussian (top) and Laplacian (bottom) as a function of the number of samples. Error bars are the variance in the estimate over 25 samples. The red/blue points are shown slightly offset for clarity}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Entropy estimation of natural image patches. Estimates (left) converge for $1\times 1$ and $2\times 2$ pixel patches. Note the drop in entropy, indicative of considerable redundancy, for increasing patch sizes. Runtime (right) approaches more than 10 hours for sample sizes of $10^6$, and appears to be growing roughly linearly on the log-log plot.}}{7}{figure.7}}
\newlabel{fig:entropy}{{7}{7}{Entropy estimation of natural image patches. Estimates (left) converge for $1\times 1$ and $2\times 2$ pixel patches. Note the drop in entropy, indicative of considerable redundancy, for increasing patch sizes. Runtime (right) approaches more than 10 hours for sample sizes of $10^6$, and appears to be growing roughly linearly on the log-log plot}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Entropy estimation of image patches projected onto PCA and ICA basis vectors. Basis vectors were learned from $32\times 32$ pixel patches using principal and independent components analysis. New samples were then projected onto this basis set and the entropy of each projection was estimated separately. Projection onto PCA bases (red) are indexed by decreasing eigenvalue, ICA indices are arbitrary. An increase in the entropy is seen from the projection onto ICA bases compared with PCA bases.}}{8}{figure.8}}
\newlabel{fig:projent}{{8}{8}{Entropy estimation of image patches projected onto PCA and ICA basis vectors. Basis vectors were learned from $32\times 32$ pixel patches using principal and independent components analysis. New samples were then projected onto this basis set and the entropy of each projection was estimated separately. Projection onto PCA bases (red) are indexed by decreasing eigenvalue, ICA indices are arbitrary. An increase in the entropy is seen from the projection onto ICA bases compared with PCA bases}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Entropy estimation of natural image patches. Estimates (left) converge for $1\times 1$ and $2\times 2$ pixel patches. Note the drop in entropy, indicative of considerable redundancy, for increasing patch sizes. Runtime (right) approaches more than 10 hours for sample sizes of $10^6$, and appears to be growing roughly linearly on the log-log plot.}}{9}{figure.9}}
\newlabel{fig:jointent}{{9}{9}{Entropy estimation of natural image patches. Estimates (left) converge for $1\times 1$ and $2\times 2$ pixel patches. Note the drop in entropy, indicative of considerable redundancy, for increasing patch sizes. Runtime (right) approaches more than 10 hours for sample sizes of $10^6$, and appears to be growing roughly linearly on the log-log plot}{figure.9}{}}
